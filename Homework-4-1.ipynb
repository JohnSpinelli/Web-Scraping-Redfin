{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Houses in Redfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"John Spinelli, U50128653\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be completed INDIVIDUALLY and due on April 10 at 7pm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will practice web scraping. Let's get some basic information for each house/apartment in Massachusetts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see houses around Boston University [here](https://www.redfin.com/zipcode/02215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Information to be scraped](redfin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each house page, scrape the information of this [house](https://www.redfin.com/MA/Boston/120-Mountfort-St-02215/unit-502/home/11838719), including \n",
    "* full address\n",
    "* price\n",
    "* number of beds\n",
    "* number of baths (full or not)\n",
    "* Sq. Ft (square feet)\n",
    "* price per Sq. Ft\n",
    "* property type\n",
    "* County\n",
    "* community\n",
    "* built\n",
    "* Property details (e.g. dishwasher, elevator, parking, heating, air conditioning, maintenance, etc.)\n",
    "\n",
    "Try to extract as many features as possible. You will use this information in future homework. \n",
    "\n",
    "\n",
    "Save the data in \"house.csv\" in the following format:\n",
    "\n",
    "full address, price, beds, baths, ......\n",
    "\n",
    "**To receive credit, you must commit house.csv to Github.** **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 10 23:18:35 2017\n",
    "\n",
    "@author: JohnSpinelli\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'JohnSpinelli'\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import random \n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as Df\n",
    "\n",
    "\n",
    "\n",
    "prefix = 'https://www.redfin.com'\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\"\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "\n",
    "path_to_chromedriver = \"/Users/JohnSpinelli/desktop/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "randTimer = random.uniform(1,4)\n",
    "time.sleep(randTimer)\n",
    "driver.get('https://www.redfin.com/zipcode/02215')\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,\"html\")\n",
    "\n",
    "# Retrieve the URLs of the 6 listings\n",
    "\n",
    "propertyUrls = []\n",
    "\n",
    "links = soup.find_all(\"a\", class_=\"link\",href=True)\n",
    "for item in links:\n",
    "    propertyUrls.append(item.get(\"href\"))\n",
    "   \n",
    "propertyUrls = list(set(propertyUrls))\n",
    "propertyUrls = [prefix + x for x in propertyUrls]\n",
    "\n",
    "\n",
    "\n",
    "# Try a property URL and extract the information needed and store in a dict\n",
    "propertyList = []\n",
    "for url in propertyUrls:\n",
    "    \n",
    "    randTimer = random.uniform(1,4)\n",
    "    time.sleep(randTimer)\n",
    "    html2 = requests.get(url,headers=headers)\n",
    "    time.sleep(randTimer)\n",
    "    soup2 = BeautifulSoup(html2.text, 'html')\n",
    "            \n",
    "        \n",
    "    # Div that contains address, price, beds, bath, sqft/sqft price\n",
    "             \n",
    "    address = soup2.find('span', {\"class\":\"street-address\"})['title']\n",
    "    mainStats = soup2.find_all('div',{'class':'main-font statsValue'})\n",
    "    price = mainStats[0].get_text()\n",
    "    beds = mainStats[1].get_text()\n",
    "    baths = mainStats[2].get_text()\n",
    "    #sqftStats = soup2.find_all('div',{'class':'info-block left-divider sqft'})\n",
    "    \n",
    "    sqft = soup2.find_all('span',{'class':'main-font statsValue'})[0].get_text()\n",
    "    sqftPrice = soup2.find_all('div',{'class':'statsLabel'})[0].get_text().split(' ')[0]\n",
    "    \n",
    "    # Div that contains property type, community, county, community, built\n",
    "    \n",
    "    keyDetails = soup2.find_all('div', {'class':'keyDetail'})\n",
    "    propertyType = keyDetails[1].get_text().split('Type')[1]\n",
    "    community = keyDetails[3].get_text().split('Community')[1]\n",
    "    county = keyDetails[4].get_text().split('County')[1]\n",
    "    built = keyDetails[6].get_text().split('Built')[1]\n",
    "\n",
    "    propertyDict = {'Address':address,'Price':price,'Beds':beds,\\\n",
    "                         'Baths':baths,'Square Feet':sqft,'Price Per Sqft':sqftPrice,\\\n",
    "                         'Property Type':propertyType,'Community':community,\\\n",
    "                         'County':county,'Built':built}\n",
    "    \n",
    "# Div that contains property details\n",
    "    \n",
    "    propertyDetails = soup2.find_all('div', {'class':'amenity-group'})\n",
    "    details = []\n",
    "    columns = []\n",
    "    for i in range(1,6):\n",
    "        x =re.findall('[A-Z][^A-Z]*',propertyDetails[i].get_text())\n",
    "        details.append(' '.join(x))\n",
    "        \n",
    "    propertyDict['Detail 1'] = details[0]\n",
    "    propertyDict['Detail 2'] = details[1]\n",
    "    propertyDict['Detail 3'] = details[2]\n",
    "    propertyDict['Detail 4'] = details[3]\n",
    "    propertyDict['Detail 5'] = details[4]\n",
    "              \n",
    "    \n",
    "    propertyList.append({'Address':address,'Price':price,'Beds':beds,\\\n",
    "                         'Baths':baths,'Square Feet':sqft,'Price Per Sqft':sqftPrice,\\\n",
    "                         'Property Type':propertyType,'Community':community,\\\n",
    "                         'County':county,'Built':built,'Detail 1':propertyDict['Detail 1'],\\\n",
    "                         'Detail 2':propertyDict['Detail 2'],'Detail 3':propertyDict['Detail 3'],\\\n",
    "                         'Detail 4':propertyDict['Detail 4'],\\\n",
    "                         'Detail 5':propertyDict['Detail 5']})\n",
    "    \n",
    "\n",
    "listingDf = Df(propertyList,columns=['Address','Price','Beds','Baths','Square Feet',\\\n",
    "               'Price Per Sqft', 'Property Type', 'Community',\\\n",
    "               'County','Built','Detail 1','Detail 2','Detail 3','Detail 4','Detail 5'])\n",
    "\n",
    "csvPath = '/Users/JohnSpinelli/desktop/house.csv'\n",
    "listingDf.to_csv(csvPath)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can scrape and extract information of a house, scrape the information of houses in Massachusetts (as of today there are around 10000 houses in MA listed in Redfin). To see all the listings, you can query all the zip codes in Massachusetts. You can see all the zip codes in Massachusetts [here](https://github.com/evimaria/CS506-Spring2007/blob/master/Homeworks/zipcodes.txt).\n",
    "Save the data in \"MA_houses.csv\", one line for each house:\n",
    "**To receive credit, you must commit MA_houses.csv to Github** **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 10 23:18:35 2017\n",
    "\n",
    "@author: JohnSpinelli\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'JohnSpinelli'\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import random \n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as Df\n",
    "\n",
    "\n",
    "zipFile = '/Users/JohnSpinelli/desktop/zipcodes.txt'\n",
    "with open(zipFile) as f: \n",
    "    zips = f.readlines()\n",
    "zips = [x.strip() for x in zips]\n",
    "\n",
    "for zip1 in zips:\n",
    "    prefix = 'https://www.redfin.com'\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\"\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    \n",
    "    path_to_chromedriver = \"/Users/JohnSpinelli/desktop/chromedriver\"\n",
    "    driver = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "    randTimer = random.uniform(1,4)\n",
    "    time.sleep(randTimer)\n",
    "    driver.get('https://www.redfin.com/zipcode/' + zip1)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,\"html\")\n",
    "    \n",
    "    # Retrieve the URLs of the 6 listings\n",
    "    driver.close()\n",
    "    propertyUrls = []\n",
    "    \n",
    "    \n",
    "    links = soup.find_all(\"a\", class_=\"link\",href=True)\n",
    "    for item in links:\n",
    "        propertyUrls.append(item.get(\"href\"))\n",
    "    \n",
    "    if (len(propertyUrls) > 1):\n",
    "        propertyUrls = list(set(propertyUrls))\n",
    "        propertyUrls = [prefix + x for x in propertyUrls]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Try a property URL and extract the information needed and store in a dict\n",
    "        propertyList = []\n",
    "        for url in propertyUrls:\n",
    "            \n",
    "            randTimer = random.uniform(1,4)\n",
    "            time.sleep(randTimer)\n",
    "            html2 = requests.get(url,headers=headers)\n",
    "            time.sleep(randTimer)\n",
    "            soup2 = BeautifulSoup(html2.text, 'html')\n",
    "                    \n",
    "                \n",
    "            # Div that contains address, price, beds, bath, sqft/sqft price\n",
    "                     \n",
    "            address = soup2.find('span', {\"class\":\"street-address\"})['title']\n",
    "            mainStats = soup2.find_all('div',{'class':'main-font statsValue'})\n",
    "            try:\n",
    "                price = mainStats[0].get_text()\n",
    "                beds = mainStats[1].get_text()\n",
    "                baths = mainStats[2].get_text()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                sqft = soup2.find_all('span',{'class':'main-font statsValue'})[0].get_text()\n",
    "                sqftPrice = soup2.find_all('div',{'class':'statsLabel'})[0].get_text().split(' ')[0]\n",
    "            except:\n",
    "                pass\n",
    "            # Div that contains property type, community, county, community, built\n",
    "            \n",
    "            keyDetails = soup2.find_all('div', {'class':'keyDetail'})\n",
    "            try:\n",
    "                propertyType = keyDetails[1].get_text().split('Type')[1]\n",
    "                community = keyDetails[3].get_text().split('Community')[1]\n",
    "                county = keyDetails[4].get_text().split('County')[1]\n",
    "                built = keyDetails[6].get_text().split('Built')[1]\n",
    "        \n",
    "                propertyDict = {'Address':address,'Price':price,'Beds':beds,\\\n",
    "                                 'Baths':baths,'Square Feet':sqft,'Price Per Sqft':sqftPrice,\\\n",
    "                                 'Property Type':propertyType,'Community':community,\\\n",
    "                                 'County':county,'Built':built}\n",
    "            \n",
    "        # Div that contains property details\n",
    "            \n",
    "                propertyDetails = soup2.find_all('div', {'class':'amenity-group'})\n",
    "                details = []\n",
    "                columns = []\n",
    "                for i in range(1,6):\n",
    "                    x =re.findall('[A-Z][^A-Z]*',propertyDetails[i].get_text())\n",
    "                    details.append(' '.join(x))\n",
    "                    \n",
    "                propertyDict['Detail 1'] = details[0]\n",
    "                propertyDict['Detail 2'] = details[1]\n",
    "                propertyDict['Detail 3'] = details[2]\n",
    "                propertyDict['Detail 4'] = details[3]\n",
    "                propertyDict['Detail 5'] = details[4]\n",
    "                          \n",
    "                \n",
    "                propertyList.append({'Address':address,'Price':price,'Beds':beds,\\\n",
    "                                     'Baths':baths,'Square Feet':sqft,'Price Per Sqft':sqftPrice,\\\n",
    "                                     'Property Type':propertyType,'Community':community,\\\n",
    "                                     'County':county,'Built':built,'Detail 1':propertyDict['Detail 1'],\\\n",
    "                                     'Detail 2':propertyDict['Detail 2'],'Detail 3':propertyDict['Detail 3'],\\\n",
    "                                     'Detail 4':propertyDict['Detail 4'],\\\n",
    "                                     'Detail 5':propertyDict['Detail 5']})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "listingDf = Df(propertyList,columns=['Address','Price','Beds','Baths','Square Feet',\\\n",
    "               'Price Per Sqft', 'Property Type', 'Community',\\\n",
    "               'County','Built','Detail 1','Detail 2','Detail 3','Detail 4','Detail 5'])\n",
    "\n",
    "csvPath = '/Users/JohnSpinelli/desktop/MA_houses.csv'\n",
    "listingDf.to_csv(csvPath)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempted to retrieve all of BHousing but could not stop from being blocked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many websites will block you if you are sending too many queries. To prevent getting block, you should wait between queries (typically 2-5 seconds). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
